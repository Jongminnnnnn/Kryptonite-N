{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 02:16:36,676] A new study created in memory with name: no-name-e1059d4d-dd9d-4baa-9e1c-5ac415b7b1ec\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:16:38,468] Trial 0 finished with value: 0.4975833333333333 and parameters: {'hidden_size1': 110, 'hidden_size2': 59, 'learning_rate': 0.02683462340578616, 'batch_size': 124, 'epochs': 9}. Best is trial 0 with value: 0.4975833333333333.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:16:43,039] Trial 1 finished with value: 0.5024166666666666 and parameters: {'hidden_size1': 162, 'hidden_size2': 48, 'learning_rate': 0.020774508044048335, 'batch_size': 47, 'epochs': 11}. Best is trial 1 with value: 0.5024166666666666.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:16:44,931] Trial 2 finished with value: 0.49525 and parameters: {'hidden_size1': 84, 'hidden_size2': 48, 'learning_rate': 1.604370957017662e-05, 'batch_size': 71, 'epochs': 7}. Best is trial 1 with value: 0.5024166666666666.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:16:51,929] Trial 3 finished with value: 0.49816666666666665 and parameters: {'hidden_size1': 199, 'hidden_size2': 42, 'learning_rate': 0.0007372076032597042, 'batch_size': 23, 'epochs': 9}. Best is trial 1 with value: 0.5024166666666666.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:16:53,611] Trial 4 finished with value: 0.5025 and parameters: {'hidden_size1': 156, 'hidden_size2': 83, 'learning_rate': 0.00020894596890306389, 'batch_size': 103, 'epochs': 7}. Best is trial 4 with value: 0.5025.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:16:57,102] Trial 5 finished with value: 0.4975833333333333 and parameters: {'hidden_size1': 155, 'hidden_size2': 56, 'learning_rate': 0.02786980575066299, 'batch_size': 128, 'epochs': 17}. Best is trial 4 with value: 0.5025.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:17:01,849] Trial 6 finished with value: 0.49733333333333335 and parameters: {'hidden_size1': 167, 'hidden_size2': 81, 'learning_rate': 1.1810793153502604e-05, 'batch_size': 89, 'epochs': 17}. Best is trial 4 with value: 0.5025.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:17:03,601] Trial 7 finished with value: 0.50175 and parameters: {'hidden_size1': 187, 'hidden_size2': 92, 'learning_rate': 1.5093303279800185e-05, 'batch_size': 91, 'epochs': 6}. Best is trial 4 with value: 0.5025.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:17:06,029] Trial 8 finished with value: 0.5078333333333334 and parameters: {'hidden_size1': 196, 'hidden_size2': 87, 'learning_rate': 0.004788168666049604, 'batch_size': 110, 'epochs': 10}. Best is trial 8 with value: 0.5078333333333334.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:17:09,377] Trial 9 finished with value: 0.49825 and parameters: {'hidden_size1': 103, 'hidden_size2': 62, 'learning_rate': 0.00017077935342842823, 'batch_size': 107, 'epochs': 17}. Best is trial 8 with value: 0.5078333333333334.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:17:12,928] Trial 10 finished with value: 0.5006666666666667 and parameters: {'hidden_size1': 61, 'hidden_size2': 22, 'learning_rate': 0.0040384597442918386, 'batch_size': 62, 'epochs': 14}. Best is trial 8 with value: 0.5078333333333334.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:17:14,222] Trial 11 finished with value: 0.49825 and parameters: {'hidden_size1': 134, 'hidden_size2': 79, 'learning_rate': 0.0003232981394493776, 'batch_size': 101, 'epochs': 5}. Best is trial 8 with value: 0.5078333333333334.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:17:17,260] Trial 12 finished with value: 0.48441666666666666 and parameters: {'hidden_size1': 143, 'hidden_size2': 91, 'learning_rate': 0.0037254047949693055, 'batch_size': 111, 'epochs': 13}. Best is trial 8 with value: 0.5078333333333334.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:17:20,518] Trial 13 finished with value: 0.49391666666666667 and parameters: {'hidden_size1': 183, 'hidden_size2': 100, 'learning_rate': 0.0001048096773188234, 'batch_size': 85, 'epochs': 9}. Best is trial 8 with value: 0.5078333333333334.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:17:22,785] Trial 14 finished with value: 0.5009166666666667 and parameters: {'hidden_size1': 175, 'hidden_size2': 74, 'learning_rate': 0.003399260125105741, 'batch_size': 117, 'epochs': 11}. Best is trial 8 with value: 0.5078333333333334.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:17:24,578] Trial 15 finished with value: 0.509 and parameters: {'hidden_size1': 199, 'hidden_size2': 71, 'learning_rate': 0.0014673594647307815, 'batch_size': 97, 'epochs': 7}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:17:29,500] Trial 16 finished with value: 0.5024166666666666 and parameters: {'hidden_size1': 197, 'hidden_size2': 70, 'learning_rate': 0.09035661751747243, 'batch_size': 76, 'epochs': 15}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:17:40,091] Trial 17 finished with value: 0.5038333333333334 and parameters: {'hidden_size1': 200, 'hidden_size2': 68, 'learning_rate': 0.0012468762728061477, 'batch_size': 40, 'epochs': 20}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:17:42,834] Trial 18 finished with value: 0.5024166666666666 and parameters: {'hidden_size1': 123, 'hidden_size2': 90, 'learning_rate': 0.007761211616669814, 'batch_size': 97, 'epochs': 11}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:17:44,695] Trial 19 finished with value: 0.49225 and parameters: {'hidden_size1': 50, 'hidden_size2': 30, 'learning_rate': 0.0011859143018353726, 'batch_size': 78, 'epochs': 8}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:17:46,727] Trial 20 finished with value: 0.5019166666666667 and parameters: {'hidden_size1': 179, 'hidden_size2': 100, 'learning_rate': 5.7297196759980146e-05, 'batch_size': 60, 'epochs': 5}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:18:08,962] Trial 21 finished with value: 0.4964166666666667 and parameters: {'hidden_size1': 198, 'hidden_size2': 67, 'learning_rate': 0.0012945367770700062, 'batch_size': 19, 'epochs': 20}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:18:21,261] Trial 22 finished with value: 0.49766666666666665 and parameters: {'hidden_size1': 186, 'hidden_size2': 72, 'learning_rate': 0.0006250979922111496, 'batch_size': 36, 'epochs': 20}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:18:26,368] Trial 23 finished with value: 0.5059166666666667 and parameters: {'hidden_size1': 171, 'hidden_size2': 85, 'learning_rate': 0.0019839825241548654, 'batch_size': 44, 'epochs': 10}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:18:28,752] Trial 24 finished with value: 0.4975833333333333 and parameters: {'hidden_size1': 173, 'hidden_size2': 86, 'learning_rate': 0.008487053920643887, 'batch_size': 117, 'epochs': 10}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:18:33,338] Trial 25 finished with value: 0.49983333333333335 and parameters: {'hidden_size1': 145, 'hidden_size2': 79, 'learning_rate': 0.001844778008622032, 'batch_size': 64, 'epochs': 12}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:18:36,892] Trial 26 finished with value: 0.4975833333333333 and parameters: {'hidden_size1': 187, 'hidden_size2': 77, 'learning_rate': 0.009483903167488268, 'batch_size': 51, 'epochs': 7}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:18:42,934] Trial 27 finished with value: 0.5048333333333334 and parameters: {'hidden_size1': 168, 'hidden_size2': 95, 'learning_rate': 0.00229327951160884, 'batch_size': 29, 'epochs': 8}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:18:45,776] Trial 28 finished with value: 0.502 and parameters: {'hidden_size1': 189, 'hidden_size2': 85, 'learning_rate': 0.0004613990198199037, 'batch_size': 95, 'epochs': 10}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:18:47,464] Trial 29 finished with value: 0.4975833333333333 and parameters: {'hidden_size1': 110, 'hidden_size2': 61, 'learning_rate': 0.01628158918281532, 'batch_size': 122, 'epochs': 8}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:18:50,633] Trial 30 finished with value: 0.5024166666666666 and parameters: {'hidden_size1': 149, 'hidden_size2': 88, 'learning_rate': 0.0513713756510553, 'batch_size': 110, 'epochs': 13}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:18:55,687] Trial 31 finished with value: 0.49666666666666665 and parameters: {'hidden_size1': 167, 'hidden_size2': 95, 'learning_rate': 0.0017425334286845992, 'batch_size': 36, 'epochs': 8}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:19:03,131] Trial 32 finished with value: 0.49883333333333335 and parameters: {'hidden_size1': 172, 'hidden_size2': 95, 'learning_rate': 0.002653600173612848, 'batch_size': 30, 'epochs': 10}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:19:06,009] Trial 33 finished with value: 0.4975833333333333 and parameters: {'hidden_size1': 158, 'hidden_size2': 84, 'learning_rate': 0.005739352017296268, 'batch_size': 50, 'epochs': 6}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:19:13,201] Trial 34 finished with value: 0.4975833333333333 and parameters: {'hidden_size1': 191, 'hidden_size2': 96, 'learning_rate': 0.01784515906218062, 'batch_size': 29, 'epochs': 9}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:19:18,875] Trial 35 finished with value: 0.5065 and parameters: {'hidden_size1': 180, 'hidden_size2': 76, 'learning_rate': 0.0007437218375181275, 'batch_size': 46, 'epochs': 12}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:19:24,300] Trial 36 finished with value: 0.49675 and parameters: {'hidden_size1': 177, 'hidden_size2': 52, 'learning_rate': 0.0008341000627964594, 'batch_size': 44, 'epochs': 12}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:19:30,922] Trial 37 finished with value: 0.4990833333333333 and parameters: {'hidden_size1': 194, 'hidden_size2': 75, 'learning_rate': 0.00042237151224032465, 'batch_size': 54, 'epochs': 15}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:19:33,830] Trial 38 finished with value: 0.5051666666666667 and parameters: {'hidden_size1': 92, 'hidden_size2': 65, 'learning_rate': 0.00019709274305224726, 'batch_size': 68, 'epochs': 11}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:19:36,323] Trial 39 finished with value: 0.5053333333333333 and parameters: {'hidden_size1': 163, 'hidden_size2': 40, 'learning_rate': 3.489887917001273e-05, 'batch_size': 84, 'epochs': 9}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:19:41,536] Trial 40 finished with value: 0.5024166666666666 and parameters: {'hidden_size1': 180, 'hidden_size2': 81, 'learning_rate': 0.004931564429882277, 'batch_size': 56, 'epochs': 14}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:19:44,051] Trial 41 finished with value: 0.5055 and parameters: {'hidden_size1': 162, 'hidden_size2': 39, 'learning_rate': 6.114233032599466e-05, 'batch_size': 81, 'epochs': 9}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:19:46,294] Trial 42 finished with value: 0.5016666666666667 and parameters: {'hidden_size1': 156, 'hidden_size2': 45, 'learning_rate': 0.00010369456925050799, 'batch_size': 102, 'epochs': 10}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:19:48,226] Trial 43 finished with value: 0.5054166666666666 and parameters: {'hidden_size1': 134, 'hidden_size2': 56, 'learning_rate': 2.6269528438034747e-05, 'batch_size': 81, 'epochs': 7}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:19:49,696] Trial 44 finished with value: 0.5074166666666666 and parameters: {'hidden_size1': 192, 'hidden_size2': 27, 'learning_rate': 0.0006999687183870902, 'batch_size': 91, 'epochs': 6}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:19:51,257] Trial 45 finished with value: 0.5036666666666667 and parameters: {'hidden_size1': 192, 'hidden_size2': 23, 'learning_rate': 0.0006482232146497009, 'batch_size': 92, 'epochs': 6}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:19:52,591] Trial 46 finished with value: 0.5003333333333333 and parameters: {'hidden_size1': 183, 'hidden_size2': 34, 'learning_rate': 0.0002821403546659837, 'batch_size': 107, 'epochs': 5}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:19:56,675] Trial 47 finished with value: 0.4975 and parameters: {'hidden_size1': 200, 'hidden_size2': 64, 'learning_rate': 0.0029864537703491972, 'batch_size': 72, 'epochs': 12}. Best is trial 15 with value: 0.509.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:19:58,832] Trial 48 finished with value: 0.5104166666666666 and parameters: {'hidden_size1': 192, 'hidden_size2': 72, 'learning_rate': 0.0008442858251215278, 'batch_size': 88, 'epochs': 7}. Best is trial 48 with value: 0.5104166666666666.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/1746643271.py:56: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-11-14 02:20:00,659] Trial 49 finished with value: 0.5025 and parameters: {'hidden_size1': 194, 'hidden_size2': 57, 'learning_rate': 0.0008836963975624147, 'batch_size': 98, 'epochs': 6}. Best is trial 48 with value: 0.5104166666666666.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_size1': 192, 'hidden_size2': 72, 'learning_rate': 0.0008442858251215278, 'batch_size': 88, 'epochs': 7}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score\n",
    "from helperFunctions import *\n",
    "import optuna\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_size = hidden_size\n",
    "        layers.append(nn.Linear(input_size, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_and_validate(model, optimizer, criterion, X_train, y_train, X_val, y_val, epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        permutation = torch.randperm(X_train.size(0))\n",
    "        for i in range(0, X_train.size(0), batch_size):\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x, batch_y = X_train[indices], y_train[indices]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # 검증 정확도 계산\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val)\n",
    "        val_predictions = torch.argmax(val_outputs, dim=1)\n",
    "        val_accuracy = accuracy_score(y_val.cpu(), val_predictions.cpu())\n",
    "        \n",
    "    return val_accuracy\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    # 하이퍼파라미터 샘플링\n",
    "    hidden_size1 = trial.suggest_int('hidden_size1', 50, 200)\n",
    "    hidden_size2 = trial.suggest_int('hidden_size2', 20, 100)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128)\n",
    "    epochs = trial.suggest_int('epochs', 5, 20)\n",
    "\n",
    "    # 데이터셋 준비 및 다항식 특징 생성\n",
    "    n = 30\n",
    "    e = 2  # 예시 차수\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = preprocess(n=n, validate=True, standardize=True)\n",
    "    poly = PolynomialFeatures(degree=e)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "\n",
    "    # 텐서 변환\n",
    "    X_train_tensor = torch.tensor(X_train_poly, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val_poly, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "    # 모델 초기화\n",
    "    input_size = X_train_poly.shape[1]\n",
    "    output_size = len(np.unique(y_train))\n",
    "    model = MLP(input_size=input_size, hidden_sizes=[hidden_size1, hidden_size2], output_size=output_size).to(device)\n",
    "    \n",
    "    # 옵티마이저 및 손실함수 정의\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 학습 및 검증 정확도 계산\n",
    "    val_accuracy = train_and_validate(model, optimizer, criterion, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, epochs, batch_size)\n",
    "    \n",
    "    return val_accuracy\n",
    "\n",
    "# Optuna study 시작\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# 최적 하이퍼파라미터 출력\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=9, e=1, Validation Accuracy=0.9603, Test Accuracy=0.9572\n",
      "n=9, e=2, Validation Accuracy=0.9597, Test Accuracy=0.9569\n",
      "n=9, e=3, Validation Accuracy=0.9553, Test Accuracy=0.9539\n",
      "n=9, e=4, Validation Accuracy=0.9533, Test Accuracy=0.9547\n",
      "n=12, e=1, Validation Accuracy=0.9490, Test Accuracy=0.9525\n",
      "n=12, e=2, Validation Accuracy=0.9413, Test Accuracy=0.9390\n",
      "n=12, e=3, Validation Accuracy=0.9340, Test Accuracy=0.9398\n",
      "n=12, e=4, Validation Accuracy=0.9306, Test Accuracy=0.9300\n",
      "n=15, e=1, Validation Accuracy=0.8763, Test Accuracy=0.8768\n",
      "n=15, e=2, Validation Accuracy=0.5238, Test Accuracy=0.5258\n",
      "n=15, e=3, Validation Accuracy=0.5635, Test Accuracy=0.5543\n",
      "n=15, e=4, Validation Accuracy=0.6070, Test Accuracy=0.6103\n",
      "n=18, e=1, Validation Accuracy=0.7796, Test Accuracy=0.7719\n",
      "n=18, e=2, Validation Accuracy=0.5143, Test Accuracy=0.5125\n",
      "n=18, e=3, Validation Accuracy=0.5082, Test Accuracy=0.5089\n",
      "n=18, e=4, Validation Accuracy=0.5008, Test Accuracy=0.5046\n",
      "n=24, e=1, Validation Accuracy=0.5024, Test Accuracy=0.5018\n",
      "n=24, e=2, Validation Accuracy=0.5050, Test Accuracy=0.5111\n",
      "n=24, e=3, Validation Accuracy=0.4971, Test Accuracy=0.5017\n",
      "n=24, e=4, Validation Accuracy=0.5039, Test Accuracy=0.5049\n",
      "n=30, e=1, Validation Accuracy=0.5043, Test Accuracy=0.4977\n",
      "n=30, e=2, Validation Accuracy=0.5021, Test Accuracy=0.4974\n",
      "n=30, e=3, Validation Accuracy=0.4963, Test Accuracy=0.5067\n",
      "n=30, e=4, Validation Accuracy=0.4958, Test Accuracy=0.4953\n",
      "Validation Accuracies by n and e: [[0.9602777777777778, 0.9597222222222223, 0.9552777777777778, 0.9533333333333334], [0.9489583333333333, 0.94125, 0.9339583333333333, 0.930625], [0.8763333333333333, 0.5238333333333334, 0.5635, 0.607], [0.7795833333333333, 0.5143055555555556, 0.5081944444444444, 0.5008333333333334], [0.5023958333333334, 0.505, 0.4970833333333333, 0.5038541666666667], [0.5043333333333333, 0.5020833333333333, 0.49633333333333335, 0.49583333333333335]]\n",
      "Test Accuracies by n and e: [[0.9572222222222222, 0.9569444444444445, 0.9538888888888889, 0.9547222222222222], [0.9525, 0.9389583333333333, 0.9397916666666667, 0.93], [0.8768333333333334, 0.5258333333333334, 0.5543333333333333, 0.6103333333333333], [0.7719444444444444, 0.5125, 0.5088888888888888, 0.5045833333333334], [0.5017708333333334, 0.5111458333333333, 0.5016666666666667, 0.5048958333333333], [0.49766666666666665, 0.4974166666666667, 0.5066666666666667, 0.49533333333333335]]\n",
      "Feature Counts by n and e: [[10, 55, 220, 715], [13, 91, 455, 1820], [16, 136, 816, 3876], [19, 190, 1330, 7315], [25, 325, 2925, 20475], [31, 496, 5456, 46376]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score\n",
    "from helperFunctions import preprocess\n",
    "\n",
    "# 실험할 n 및 e 값 설정\n",
    "possible_n_vals = [9, 12, 15, 18, 24, 30]\n",
    "possible_e_vals = [1, 2, 3, 4]\n",
    "\n",
    "# 성능 결과를 저장할 리스트\n",
    "val_accuracies = []\n",
    "test_accuracies = []\n",
    "feature_counts = []\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 실험 시작\n",
    "for n in possible_n_vals:\n",
    "    single_val_accuracies = []\n",
    "    single_test_accuracies = []\n",
    "    single_feature_counts = []\n",
    "    \n",
    "    for e in possible_e_vals:\n",
    "        # 데이터셋 로드 및 전처리\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = preprocess(n=n, validate=True, standardize=True)\n",
    "\n",
    "        # 다항식 특징 생성\n",
    "        poly = PolynomialFeatures(degree=e)\n",
    "        X_train_poly = poly.fit_transform(X_train)\n",
    "        X_val_poly = poly.transform(X_val)\n",
    "        X_test_poly = poly.transform(X_test)\n",
    "\n",
    "        # 텐서로 변환\n",
    "        X_train_tensor = torch.tensor(X_train_poly, dtype=torch.float32).to(device)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "        X_val_tensor = torch.tensor(X_val_poly, dtype=torch.float32).to(device)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "        X_test_tensor = torch.tensor(X_test_poly, dtype=torch.float32).to(device)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "        # 모델 초기화\n",
    "        model = MLP(input_size=X_train_poly.shape[1], num_classes=len(np.unique(y_train))).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        # 학습\n",
    "        epochs = 10\n",
    "        batch_size = 32\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            permutation = torch.randperm(X_train_tensor.size(0))\n",
    "            for i in range(0, X_train_tensor.size(0), batch_size):\n",
    "                indices = permutation[i:i+batch_size]\n",
    "                batch_x, batch_y = X_train_tensor[indices], y_train_tensor[indices]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # 검증 및 테스트 데이터 평가\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_predictions = torch.argmax(val_outputs, dim=1).cpu().numpy()\n",
    "            val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "            \n",
    "            test_outputs = model(X_test_tensor)\n",
    "            test_predictions = torch.argmax(test_outputs, dim=1).cpu().numpy()\n",
    "            test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "        \n",
    "        # 생성된 특징 수 계산 및 결과 저장\n",
    "        features = X_train_poly.shape[1]\n",
    "        single_val_accuracies.append(val_accuracy)\n",
    "        single_test_accuracies.append(test_accuracy)\n",
    "        single_feature_counts.append(features)\n",
    "        \n",
    "        print(f\"n={n}, e={e}, Validation Accuracy={val_accuracy:.4f}, Test Accuracy={test_accuracy:.4f}\")\n",
    "\n",
    "    # 각 n에 대한 성능과 특징 수 저장\n",
    "    val_accuracies.append(single_val_accuracies)\n",
    "    test_accuracies.append(single_test_accuracies)\n",
    "    feature_counts.append(single_feature_counts)\n",
    "\n",
    "# 최종 결과 출력\n",
    "print(\"Validation Accuracies by n and e:\", val_accuracies)\n",
    "print(\"Test Accuracies by n and e:\", test_accuracies)\n",
    "print(\"Feature Counts by n and e:\", feature_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 02:40:55,148] A new study created in memory with name: no-name-261beada-ada3-49ab-8d32-c4266e2582d7\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/475529609.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
      "[I 2024-11-14 02:40:58,095] Trial 0 finished with value: 0.5008333333333334 and parameters: {'hidden_layer1': 64, 'hidden_layer2': 70, 'lr': 0.0001510689264056825, 'batch_size': 125, 'epochs': 23}. Best is trial 0 with value: 0.5008333333333334.\n",
      "/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/475529609.py:44: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
      "[W 2024-11-14 02:41:03,952] Trial 1 failed with parameters: {'hidden_layer1': 126, 'hidden_layer2': 42, 'lr': 0.0015156892665691432, 'batch_size': 22, 'epochs': 17} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/kryptonite_env/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/cc/95y4d8f130j8q9tr4sn6xvmr0000gn/T/ipykernel_58716/475529609.py\", line 62, in objective\n",
      "    optimizer.zero_grad()\n",
      "  File \"/opt/anaconda3/envs/kryptonite_env/lib/python3.9/site-packages/torch/_compile.py\", line 25, in inner\n",
      "    disable_fn = getattr(fn, \"__dynamo_disable\", None)\n",
      "KeyboardInterrupt\n",
      "[W 2024-11-14 02:41:03,953] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 97\u001b[0m\n\u001b[1;32m     93\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m possible_e_vals:\n\u001b[0;32m---> 97\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# 다항식 특징 생성\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     poly \u001b[38;5;241m=\u001b[39m PolynomialFeatures(degree\u001b[38;5;241m=\u001b[39me)\n",
      "Cell \u001b[0;32mIn[5], line 78\u001b[0m, in \u001b[0;36moptimize_hyperparameters\u001b[0;34m(n, e)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Optuna 최적화 실행\u001b[39;00m\n\u001b[1;32m     77\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 78\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# 최적의 하이퍼파라미터 반환\u001b[39;00m\n\u001b[1;32m     81\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kryptonite_env/lib/python3.9/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kryptonite_env/lib/python3.9/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kryptonite_env/lib/python3.9/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kryptonite_env/lib/python3.9/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kryptonite_env/lib/python3.9/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[5], line 62\u001b[0m, in \u001b[0;36moptimize_hyperparameters.<locals>.objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     59\u001b[0m indices \u001b[38;5;241m=\u001b[39m permutation[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     60\u001b[0m batch_x, batch_y \u001b[38;5;241m=\u001b[39m X_train_tensor[indices]\u001b[38;5;241m.\u001b[39mto(device), y_train_tensor[indices]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 62\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_x)\n\u001b[1;32m     64\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/kryptonite_env/lib/python3.9/site-packages/torch/_compile.py:25\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# cache this on the first invocation to avoid adding too much overhead.\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__dynamo_disable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score\n",
    "from helperFunctions import *\n",
    "\n",
    "\"\"\"\n",
    "parameter tuner\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Define MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_size = hidden_size\n",
    "        layers.append(nn.Linear(input_size, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def optimize_hyperparameters(n, e):\n",
    "    # 데이터 전처리\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = preprocess(n=n, validate=True, standardize=True)\n",
    "    poly = PolynomialFeatures(degree=e)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train_poly, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_val_tensor = torch.tensor(X_val_poly, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def objective(trial):\n",
    "        # 하이퍼파라미터 샘플링\n",
    "        hidden_layer1 = trial.suggest_int('hidden_layer1', 50, 200)\n",
    "        hidden_layer2 = trial.suggest_int('hidden_layer2', 25, 100)\n",
    "        lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "        batch_size = trial.suggest_int('batch_size', 16, 128)\n",
    "        epochs = trial.suggest_int('epochs', 5, 30)\n",
    "\n",
    "        # 모델 초기화\n",
    "        model = MLP(input_size=X_train_poly.shape[1], hidden_sizes=[hidden_layer1, hidden_layer2],\n",
    "                    output_size=len(np.unique(y_train))).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # 학습\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            permutation = torch.randperm(X_train_tensor.size(0))\n",
    "            for i in range(0, X_train_tensor.size(0), batch_size):\n",
    "                indices = permutation[i:i+batch_size]\n",
    "                batch_x, batch_y = X_train_tensor[indices].to(device), y_train_tensor[indices].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # 검증 데이터 평가\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor.to(device))\n",
    "            val_predictions = torch.argmax(val_outputs, dim=1)\n",
    "            val_accuracy = accuracy_score(y_val, val_predictions.cpu().numpy())\n",
    "        return val_accuracy\n",
    "\n",
    "    # Optuna 최적화 실행\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    # 최적의 하이퍼파라미터 반환\n",
    "    best_params = study.best_params\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "    return best_params\n",
    "\n",
    "\n",
    "# preprocess dataset for\n",
    "n = 30\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = preprocess(n=n, validate=True, standardize=True)\n",
    "\n",
    "\n",
    "#degree\n",
    "possible_e_vals = [1]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for e in possible_e_vals:\n",
    "\n",
    "    best_params = optimize_hyperparameters(n, e)\n",
    "    # 다항식 특징 생성\n",
    "    poly = PolynomialFeatures(degree=e)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "\n",
    "    # 텐서로 변환\n",
    "    X_train_tensor = torch.tensor(X_train_poly, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val_poly, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test_poly, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    # 모델 초기화\n",
    "    model = MLP(input_size=X_train_poly.shape[1],\n",
    "            hidden_sizes=[best_params['hidden_layer1'], best_params['hidden_layer2']],\n",
    "            output_size=len(np.unique(y_train))).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "\n",
    "    # 학습\n",
    "    for epoch in range(best_params['epochs']):\n",
    "        model.train()\n",
    "        permutation = torch.randperm(X_train_tensor.size(0))\n",
    "        for i in range(0, X_train_tensor.size(0), best_params['batch_size']):\n",
    "            indices = permutation[i:i+best_params['batch_size']]\n",
    "            batch_x, batch_y = X_train_tensor[indices], y_train_tensor[indices]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # 검증 및 테스트 데이터 평가\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_predictions = torch.argmax(val_outputs, dim=1).cpu().numpy()\n",
    "            val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "            \n",
    "            test_outputs = model(X_test_tensor)\n",
    "            test_predictions = torch.argmax(test_outputs, dim=1).cpu().numpy()\n",
    "            test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "        \n",
    "        # 생성된 특징 수 계산 및 결과 저장\n",
    "        \n",
    "    print(f\"n={n}, e={e}, Validation Accuracy={val_accuracy:.4f}, Test Accuracy={test_accuracy:.4f}\")\n",
    "targetAcc = targetPerformance(n)\n",
    "print(f\"Target Accuracy:{targetAcc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kryptonite_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
